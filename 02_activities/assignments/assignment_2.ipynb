{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c6788aef474ca12",
   "metadata": {
    "collapsed": false,
    "id": "7c6788aef474ca12"
   },
   "source": [
    "# Text Generation with Recurrent Neural Networks (RNNs)\n",
    "\n",
    "In this assignment, you'll build upon your understanding of RNNs and Keras to develop a word-level text generation model.  Your goal is to train a model that learns the stylistic nuances of a chosen corpus and generates new, original text segments that echo the source material's essence.\n",
    "\n",
    "**Datasets**\n",
    "\n",
    "We've provided several intriguing text corpora to get you started:\n",
    "\n",
    "*   Mark Twain\n",
    "*   Charles Dickens\n",
    "*   William Shakespeare\n",
    "\n",
    "**Feel free to explore!**  If you have a particular passion for another author, genre, or a specific text, you're encouraged to use your own dataset of raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2d0bfedcfe52aedc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2d0bfedcfe52aedc",
    "outputId": "eef898b3-cf85-43d1-9715-30194b9932d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available. If you're on Colab, go to Runtime > Change runtime and select a GPU hardware accelerator.\n"
     ]
    }
   ],
   "source": [
    "# Check if we have a GPU available\n",
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"No GPU available. If you're on Colab, go to Runtime > Change runtime and select a GPU hardware accelerator.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "54819e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Project Gutenberg EBook of The Prince and The Pauper, Complete by\n",
      "Mark Twain (Samuel Clemens)\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with almost\n",
      "no restrictions whatsoever. You may copy it, give it away or re-use\n",
      "it under the terms of the Project Gutenberg License included with this\n",
      "eBook or online at www.gutenberg.org\n",
      "\n",
      "Title: The Prince and The Pauper, Complete\n",
      "\n",
      "Author: Mark Twain (Samuel Clemens)\n",
      "\n",
      "Release Date: August 20, 2006 [EBook #1837]\n",
      "Last Updated: February 19, 2018\n",
      "\n",
      "Language: English\n",
      "\n",
      "Character set encoding: UTF-8\n",
      "\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK PRINCE AND THE PAUPER ***\n",
      "\n",
      "Produced by David Widger. The earliest PG edition was prepared by Les\n",
      "Bowler\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THE PRINCE AND THE PAUPER\n",
      "\n",
      "by Mark Twain\n",
      "\n",
      "The Great Seal\n",
      "\n",
      "I will set down a tale as it was told to me by one who had it of his\n",
      "father, which latter had it of HIS father, this last having in like\n",
      "manner had it of HIS father--and so on, back and still back, three\n",
      "hundred years and more, the fat\n"
     ]
    }
   ],
   "source": [
    "# Open the file in read mode ('r')\n",
    "with open('mark_twain.txt', 'r', encoding='utf-8') as f:\n",
    "    # Step 2: Read the content of the file\n",
    "    text = f.read()\n",
    "\n",
    "# Use a fraction to get only part of the text\n",
    "fraction = 0.1  # Set to 0.1 to load the first 10% of the file\n",
    "text = text[:int(fraction * len(text))]\n",
    "\n",
    "# Now the entire content of the text file is stored in the 'text' variable\n",
    "print(text[:1000])  # Print the first 1000 characters to see a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9c28c497f620b775",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T21:30:44.959803Z",
     "start_time": "2024-02-08T21:30:44.701343Z"
    },
    "id": "9c28c497f620b775"
   },
   "outputs": [],
   "source": [
    "# def download_file(url, file_path):\n",
    "#     import requests\n",
    "#     r = requests.get(url)\n",
    "#     with open(file_path, 'wb') as f:\n",
    "#         f.write(r.content)\n",
    "\n",
    "# def load_dataset(file_path, fraction=1.0):\n",
    "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#         raw_text = f.read()\n",
    "#     return raw_text[:int(fraction * len(raw_text))]\n",
    "\n",
    "# dataset = 'shakespeare.txt' # Other options are mark_twain.txt, charles_dickens.txt\n",
    "\n",
    "# download_file(f'https://github.com/UofT-DSI/deep_learning/raw/main/assignments/downloaded_books/' + dataset, dataset)\n",
    "\n",
    "# # Load chosen dataset. NOTE: If Colab is running out of memory, change the `fraction` parameter to a value between 0 and 1 to load less data.\n",
    "# text = load_dataset(dataset, fraction=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab51c764031e606",
   "metadata": {
    "collapsed": false,
    "id": "dab51c764031e606"
   },
   "source": [
    "# 1. Data Preparation (Complete or Incomplete)\n",
    "\n",
    "Before we can begin training an RNN model, we need to prepare the dataset. This involves cleaning the text, tokenizing words, and creating sequences the model can be trained on.\n",
    "\n",
    "## 1.1 Data Exploration\n",
    "\n",
    "Print the first 1000 characters of the dataset. Report the dataset's size and the number of unique characters it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "BunkZmdkl0Wn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BunkZmdkl0Wn",
    "outputId": "e429a49a-9334-4484-bd6b-2ce40c8298c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 1000 characters of the dataset:\n",
      "\n",
      "\n",
      "The Project Gutenberg EBook of The Prince and The Pauper, Complete by\n",
      "Mark Twain (Samuel Clemens)\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with almost\n",
      "no restrictions whatsoever. You may copy it, give it away or re-use\n",
      "it under the terms of the Project Gutenberg License included with this\n",
      "eBook or online at www.gutenberg.org\n",
      "\n",
      "Title: The Prince and The Pauper, Complete\n",
      "\n",
      "Author: Mark Twain (Samuel Clemens)\n",
      "\n",
      "Release Date: August 20, 2006 [EBook #1837]\n",
      "Last Updated: February 19, 2018\n",
      "\n",
      "Language: English\n",
      "\n",
      "Character set encoding: UTF-8\n",
      "\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK PRINCE AND THE PAUPER ***\n",
      "\n",
      "Produced by David Widger. The earliest PG edition was prepared by Les\n",
      "Bowler\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THE PRINCE AND THE PAUPER\n",
      "\n",
      "by Mark Twain\n",
      "\n",
      "The Great Seal\n",
      "\n",
      "I will set down a tale as it was told to me by one who had it of his\n",
      "father, which latter had it of HIS father, this last having in like\n",
      "manner had it of HIS father--and so on, back and still back, three\n",
      "hundred years and more, the fat\n",
      "\n",
      "Size of the dataset: 1267477 characters\n",
      "Number of unique characters in the dataset: 90\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "# Step 1: Print the first 1000 characters\n",
    "print(\"First 1000 characters of the dataset:\\n\")\n",
    "print(text[:1000])\n",
    "\n",
    "# Step 2: Report the size of the dataset\n",
    "dataset_size = len(text)\n",
    "print(f\"\\nSize of the dataset: {dataset_size} characters\")\n",
    "\n",
    "# Step 3: Report the number of unique characters in the dataset\n",
    "unique_chars = set(text)\n",
    "print(f\"Number of unique characters in the dataset: {len(unique_chars)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae1639f5ecfe587",
   "metadata": {
    "collapsed": false,
    "id": "3ae1639f5ecfe587"
   },
   "source": [
    "## 1.2 Text Pre-Processing\n",
    "\n",
    "To prepare the dataset for training, we need to clean the text and create a numerical representation the model can interpret. Perform the following pre-processing steps:\n",
    "\n",
    "*   Convert the entire text to lowercase.\n",
    "*   Use the `Tokenizer` class from the `keras.preprocessing.text` module to tokenize the text. You should fit the tokenizer on the text and then convert the text to a sequence of numbers. You can use the `texts_to_sequences` method to do this.\n",
    "\n",
    "**Note**:\n",
    "* You'll need to specify an appropriate size for the vocabulary. The number of words in the list of most common words can serve as a guide - does it seem like a reasonable vocabulary size?\n",
    "* Some of the words will be excluded from the vocabulary, as they don't appear often enough. It's important to provide a value for `oov_token` when creating the Tokenizer instance, so that these words can be represented as \"unknown\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4d0d30cd98ea453c",
   "metadata": {
    "id": "4d0d30cd98ea453c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 tokens in the sequence: [2, 163, 151, 929, 4, 2, 84, 3, 2, 627]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Your code here\n",
    "# Step 1: Convert text to lower case \n",
    "text = text.lower()\n",
    "\n",
    "# Step 2: Set up the Tokenizer\n",
    "vocab_size = 5000  # You can adjust this based on the most common words.\n",
    "oov_token = \"<OOV>\"  # This token will represent out-of-vocabulary words.\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
    "\n",
    "# Step 3: Fit the tokenizer on the text\n",
    "tokenizer.fit_on_texts([text])\n",
    "\n",
    "# Step 4: Convert the text to sequences\n",
    "sequences = tokenizer.texts_to_sequences([text])\n",
    "\n",
    "# Let's explore the tokenized output\n",
    "print(f\"First 10 tokens in the sequence: {sequences[0][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d32bb9356f711",
   "metadata": {
    "collapsed": false,
    "id": "89d32bb9356f711"
   },
   "source": [
    "If everything worked, the following line should show you the first 10 words in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6a7cd547a19feece",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6a7cd547a19feece",
    "outputId": "1f52c24c-fe9d-4074-a6b9-2be1b36a172e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<OOV>', 1), ('the', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('in', 7), ('was', 8), ('he', 9), ('it', 10)]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(list(tokenizer.word_index.items())[:10])\n",
    "except AttributeError:\n",
    "    print(\"Tokenizer has not been initialized. Possible issue: Complete the relevant section of the assignment to initialize it.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da504e4bc6617613",
   "metadata": {
    "collapsed": false,
    "id": "da504e4bc6617613"
   },
   "source": [
    "## 1.3 Sequence Generation\n",
    "\n",
    "Now that the text has been tokenized, we need to create sequences the model can be trained on. There are two parts to this:\n",
    "\n",
    "*   Use the `texts_to_sequences` method from the tokenizer to convert the text to a list of sequences of numbers.\n",
    "*   Generate the training sequences. Each training sequence should contain `SEQ_LENGTH` token IDs from the text. The target token for each sequence should be the word that follows the sequence in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4ff5fc8d0273709c",
   "metadata": {
    "id": "4ff5fc8d0273709c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Input Sequence: [   2  163  151  929    4    2   84    3    2  627 1216   24  582  776\n",
      " 2261 2747   22  929   20   16]\n",
      "Target Word for the Sequence: 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: Define SEQ length \n",
    "SEQ_LENGTH = 20 # Choose an appropriate sequence length\n",
    "\n",
    "# Step 2: Generate Training Sequences\n",
    "sequences = sequences[0]  # Flatten the nested list\n",
    "input_sequences = []\n",
    "target_words = []\n",
    "\n",
    "# Iterate through the entire list of tokens to create sequences of length SEQ_LENGTH\n",
    "for i in range(SEQ_LENGTH, len(sequences)):\n",
    "    # Extract sequence of SEQ_LENGTH tokens\n",
    "    input_sequence = sequences[i - SEQ_LENGTH:i]\n",
    "    # Append input sequence to the list of input sequences\n",
    "    input_sequences.append(input_sequence)\n",
    "    # The target word is the token immediately after the sequence\n",
    "    target_words.append(sequences[i])\n",
    "\n",
    "# Convert lists to numpy arrays for training\n",
    "input_sequences = np.array(input_sequences)\n",
    "target_words = np.array(target_words)\n",
    "\n",
    "# Print a sample\n",
    "print(\"Sample Input Sequence:\", input_sequences[0])\n",
    "print(\"Target Word for the Sequence:\", target_words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6bdc0deb930df1",
   "metadata": {
    "collapsed": false,
    "id": "3b6bdc0deb930df1"
   },
   "source": [
    "Assuming your sequences are stored in `X` and the corresponding targets in `y`, the following line should print the first training sequence and its target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a495cab04001ce92",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a495cab04001ce92",
    "outputId": "ecc2c1f4-81d5-4edb-ba86-8c53f86df70b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence: [   2  163  151  929    4    2   84    3    2  627 1216   24  582  776\n",
      " 2261 2747   22  929   20   16]\n",
      "Target: 2\n",
      "Translated back to words: ['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'prince', 'and', 'the', 'pauper', 'complete', 'by', 'mark', 'twain', 'samuel', 'clemens', 'this', 'ebook', 'is', 'for'] -> the\n"
     ]
    }
   ],
   "source": [
    "if len(input_sequences) > 0 and len(target_words) > 0:\n",
    "    print(f'Sequence: {input_sequences[0]}\\nTarget: {target_words[0]}')\n",
    "    print(f'Translated back to words: {[tokenizer.index_word[i] for i in input_sequences[0]]} -> {tokenizer.index_word[target_words[0]]}')\n",
    "else:\n",
    "    print(\"Training sequences have not been generated. Possible issue: Complete the relevant section of the assignment to initialize it.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bb2c55da17aaa0",
   "metadata": {
    "collapsed": false,
    "id": "d5bb2c55da17aaa0"
   },
   "source": [
    "And the following code will transform y into a one-hot encoded matrix, and split everything into training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3a929b2e6c2cc921",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3a929b2e6c2cc921",
    "outputId": "bbac2f5d-5ec3-40d0-fdd7-c567088569d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (171642, 20)\n",
      "y_train shape: (171642, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "# Ensure that tokenizer has been initialized\n",
    "if tokenizer is not None:\n",
    "    # Convert X and y to numpy arrays\n",
    "    X = np.array(input_sequences)\n",
    "    y = np.array(target_words)\n",
    "\n",
    "    # One last thing: let's drop any examples where the target is the OOV token - we don't want our model to predict that (boring!)\n",
    "    if oov_token in tokenizer.word_index:\n",
    "        mask = y != tokenizer.word_index[oov_token]\n",
    "        X = X[mask]\n",
    "        y = y[mask]\n",
    "\n",
    "    # One-hot encode the target token\n",
    "    y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    print(f'X_train shape: {X_train.shape}')\n",
    "    print(f'y_train shape: {y_train.shape}')\n",
    "else:\n",
    "    print(\"Tokenizer has not been initialized. Please initialize it and load the vocabulary before continuing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e4161897210434",
   "metadata": {
    "collapsed": false,
    "id": "b6e4161897210434"
   },
   "source": [
    "# 2. Model Development (Complete or Incomplete)\n",
    "\n",
    "With the dataset prepared, it's time to develop the RNN model. You'll need to define the architecture of the model, compile it, and prepare it for training.\n",
    "\n",
    "## 2.1 Model Architecture\n",
    "\n",
    "Define the architecture of your RNN model. You can design it however you like, but there are a few features that it's important to include:\n",
    "\n",
    "*   An embedding layer that learns a dense representation of the input tokens. You'll need to specify the input dimension (the size of the vocabulary) and the output dimension (the size of the dense representation). Remember, you can look at the documentation [here](https://keras.io/api/layers/core_layers/embedding/).\n",
    "*   At least one recurrent layer. We have learned how to use LSTM layers in class, but you can use other types of recurrent layers if you prefer. You can find the documentation [here](https://keras.io/api/layers/recurrent_layers/lstm/).\n",
    "*   A dense layer with a softmax activation function. This layer will output a probability distribution over the vocabulary, so that the model can make predictions about the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9fdfaad93818fc8d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fdfaad93818fc8d",
    "outputId": "ae02a273-ab37-4274-d41c-b2c0c80350d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 20, 100)           500000    \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 150)               150600    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5000)              755000    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,405,600\n",
      "Trainable params: 1,405,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Step 1: Set Model Parameters\n",
    "vocab_size = 5000  # Size of the vocabulary\n",
    "embedding_dim = 100  # Number of dimensions for word embeddings\n",
    "lstm_units = 150  # Number of units in LSTM layer\n",
    "seq_length = SEQ_LENGTH  # The length of input sequences\n",
    "\n",
    "# Step 2: Define the Model Architecture\n",
    "model = Sequential()\n",
    "\n",
    "# Add Embedding Layer\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=seq_length))\n",
    "\n",
    "# Add LSTM Layer\n",
    "model.add(LSTM(units=lstm_units, return_sequences=False))  # Set return_sequences=False to pass only final output to the next layer\n",
    "\n",
    "# Add Dense Output Layer with Softmax Activation\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "# Check if the model has layers before trying to print the summary\n",
    "if len(model.layers) > 0:\n",
    "    model.summary()\n",
    "else:\n",
    "    print(\"No layers have been added to the model. Please complete the assignment by adding the required layers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fafd2dbb0d589fc",
   "metadata": {
    "collapsed": false,
    "id": "2fafd2dbb0d589fc"
   },
   "source": [
    "## 2.2 Model Compilation\n",
    "\n",
    "Compile the model with an appropriate loss function and optimizer. You might also want to track additional metrics, such as accuracy.\n",
    "\n",
    "Give a short explanation of your choice of loss function and optimizer:\n",
    "\n",
    "_your explanation here_\n",
    "\n",
    "We use categorical_crossentropy because the modelâ€™s output is a probability distribution over multiple classes (words in the vocabulary). This loss function helps compare the predicted probability distribution to the true one-hot encoded labels, allowing the model to learn effectively.\n",
    "\n",
    "Adam is an adaptive optimizer that combines the advantages of SGD with momentum and RMSProp. It automatically adjusts the learning rate during training, which helps speed up convergence and often results in a better final model. The learning rate is set to 0.001, a commonly used value for initial experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ae4ca7a12051b1fd",
   "metadata": {
    "id": "ae4ca7a12051b1fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled successfully!\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(learning_rate=0.001),  # Using Adam optimizer with default learning rate\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Model compiled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f0b90a448c4f4b",
   "metadata": {
    "collapsed": false,
    "id": "c2f0b90a448c4f4b"
   },
   "source": [
    "## 2.3 Model Training\n",
    "\n",
    "Train the model on the training data you've prepared.\n",
    "\n",
    "* Train your model for 5 epochs with a batch size of 128. Use the validation data for validation.\n",
    "* Store the training history in a variable called `history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "256b1ea138c67ef7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "256b1ea138c67ef7",
    "outputId": "fba36993-0bf3-472e-f1c3-1da548c12ead"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1341/1341 [==============================] - 123s 88ms/step - loss: 6.3295 - accuracy: 0.0825 - val_loss: 6.0249 - val_accuracy: 0.1056\n",
      "Epoch 2/5\n",
      "1341/1341 [==============================] - 118s 88ms/step - loss: 5.8342 - accuracy: 0.1144 - val_loss: 5.7425 - val_accuracy: 0.1175\n",
      "Epoch 3/5\n",
      "1341/1341 [==============================] - 129s 96ms/step - loss: 5.5495 - accuracy: 0.1299 - val_loss: 5.5563 - val_accuracy: 0.1332\n",
      "Epoch 4/5\n",
      "1341/1341 [==============================] - 119s 89ms/step - loss: 5.3136 - accuracy: 0.1441 - val_loss: 5.4168 - val_accuracy: 0.1409\n",
      "Epoch 5/5\n",
      "1341/1341 [==============================] - 118s 88ms/step - loss: 5.1161 - accuracy: 0.1528 - val_loss: 5.3285 - val_accuracy: 0.1484\n",
      "Model training complete!\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "# Train the model for 5 epochs with a batch size of 128\n",
    "history = model.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=5, \n",
    "    batch_size=128, \n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "\n",
    "# Print a message after training is complete\n",
    "print(\"Model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195c59bf80d2a2c4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Plot the training history to visualize the model's learning progress. Your plot should include the training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9e8cacec70d8f313",
   "metadata": {
    "id": "9e8cacec70d8f313"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Your code here\n",
    "\n",
    "# Extract the training and validation loss from the history object\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Plotting the training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_loss, label='Training Loss', color='blue')\n",
    "plt.plot(val_loss, label='Validation Loss', color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9ed71305787aed",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3. Text Generation (Complete or Incomplete)\n",
    "\n",
    "Write a method called `generate_text` that uses the trained model to generate new text. The method should take the following parameters:\n",
    "\n",
    "*   `model`: The trained RNN model.\n",
    "*   `tokenizer`: The tokenizer used to pre-process the text data.\n",
    "*   `seed_text`: The seed text the model will use to generate new text.\n",
    "*   `max_sequence_len`: The maximum length of the sequence used to generate new text.\n",
    "\n",
    "The method should return the generated text.\n",
    "\n",
    "An overview of the text generation process you should follow:\n",
    "\n",
    "1. Tokenize the seed text using the tokenizer we built before.\n",
    "2. Pad the sequences to the same length as the training sequences - you can use the `pad_sequences` method from the `keras.preprocessing.sequence` module, which is documented [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences).\n",
    "3. Use the model to predict the next token in the sequence. Remember that the model will output a probability distribution over the vocabulary, so you'll need to use `np.argmax` to find the token with the highest probability.\n",
    "4. Add the predicted token to the sequence and remove the first token.\n",
    "5. Repeat steps 3-4 until you have generated the desired number of tokens.\n",
    "6. Convert the generated token IDs back to words and return the combined result as a single string.\n",
    "\n",
    "This is a challenging task, so don't hesitate to ask for help if you need it. It's okay if the generated text doesn't make much sense yet - we'll work on improving the model next.\n",
    "As a bonus, you can make your method generate \"gpt-style\" by having it print out each word as it's generated, so you can see the text being generated in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d73dbf278a1265ef",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Helper function to get predicted word from the model\n",
    "def get_predicted_word(model, sequence, vocab_size, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Get the predicted word from the model with a bit of randomness (temperature).\n",
    "    \"\"\"\n",
    "    # Use the model to predict the next token in the sequence\n",
    "    yhat = model.predict(sequence, verbose=0)\n",
    "    \n",
    "    # Apply temperature to predictions to introduce randomness\n",
    "    yhat = yhat.flatten()\n",
    "    yhat = np.log(yhat + 1e-8) / temperature\n",
    "    exp_preds = np.exp(yhat)\n",
    "    yhat = exp_preds / np.sum(exp_preds)\n",
    "\n",
    "    # Get the index of the predicted word by sampling from the distribution\n",
    "    if len(yhat) != vocab_size:\n",
    "        vocab_size = len(yhat)\n",
    "    predicted_index = np.random.choice(range(vocab_size), p=yhat)\n",
    "    \n",
    "    return predicted_index\n",
    "\n",
    "# Function to generate new text using the trained model\n",
    "def generate_text(model, tokenizer, seed_text, max_sequence_len, num_words_to_generate=50):\n",
    "    \"\"\"\n",
    "    Generate new text using the trained model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The trained RNN model.\n",
    "    - tokenizer: The tokenizer used to pre-process the text data.\n",
    "    - seed_text: The initial text the model will use to generate new text.\n",
    "    - max_sequence_len: The maximum length of the sequence used to generate new text.\n",
    "    - num_words_to_generate: The number of words to generate.\n",
    "    \n",
    "    Returns:\n",
    "    - The generated text.\n",
    "    \"\"\"\n",
    "    # Start with the seed text\n",
    "    generated_text = seed_text\n",
    "    \n",
    "    # Generate num_words_to_generate words\n",
    "    for _ in range(num_words_to_generate):\n",
    "        # Tokenize the seed text\n",
    "        token_list = tokenizer.texts_to_sequences([generated_text])[0]\n",
    "        \n",
    "        # Pad the sequence to match the max_sequence_len\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len, padding='pre')\n",
    "        \n",
    "        # Predict the next word\n",
    "        predicted_index = get_predicted_word(model, token_list, len(tokenizer.word_index) + 1)\n",
    "        \n",
    "        # Stop if the predicted word is the out-of-vocabulary token\n",
    "        if predicted_index == tokenizer.word_index.get('<OOV>'):\n",
    "            break\n",
    "        \n",
    "        # Get the word corresponding to the predicted index\n",
    "        predicted_word = tokenizer.index_word.get(predicted_index, '')\n",
    "        \n",
    "        # Append the predicted word to the generated text\n",
    "        generated_text += ' ' + predicted_word\n",
    "        \n",
    "        # Optionally, print each word as it is generated (for a \"gpt-style\" experience)\n",
    "        print(predicted_word, end=' ', flush=True)\n",
    "    \n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f463b0c3df49e2c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my few day i must know me affairs â€ i could had been warm Â i am innocent and hold to illustrious thing to him that it would hear me it is as safe of my of state â€ a fatherâ€™s massed of treachery upon them another years and decaying eyes "
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hamlet my few day i must know me affairs â€ i could had been warm \\xa0i am innocent and hold to illustrious thing to him that it would hear me it is as safe of my of state â€ a fatherâ€™s massed of treachery upon them another years and decaying eyes'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the text generation function\n",
    "generate_text(model, tokenizer, 'hamlet', SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5871d836a0135c41",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It's likely that the text generated by your model doesn't make much sense yet. This is because the model hasn't been trained for very long, and the training dataset is relatively small. \n",
    "\n",
    "# 4. Model Refinement (Complete or Incomplete)\n",
    "\n",
    "In this last section, you'll work on improving your model. There are many ways to do this, but here are a few ideas to get you started:\n",
    "\n",
    "* Use pre-trained embeddings: the code below will help you to load pre-trained embeddings through Keras. \n",
    "* Experiment with different model architectures, including the number of layers, the number of units in each layer, and the use of dropout layers.\n",
    "* Train your model for longer. You can also experiment with different batch sizes.\n",
    "\n",
    "Implement and test out at least one of these ideas. If you have other ideas for improving the model, feel free to try them out as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dda8b0f845c20862",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "unzip:  cannot find or open glove.6B.zip, glove.6B.zip.zip or glove.6B.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "!wget https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2466fe81",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to download GloVe embeddings.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Example usage to download GloVe embeddings\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[43mdownload_glove_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[85], line 14\u001b[0m, in \u001b[0;36mdownload_glove_embeddings\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mDownloads and extracts GloVe embeddings from Stanford's website.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     13\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 14\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m zipfile\u001b[38;5;241m.\u001b[39mZipFile(BytesIO(response\u001b[38;5;241m.\u001b[39mcontent)) \u001b[38;5;28;01mas\u001b[39;00m zfile:\n",
      "File \u001b[1;32mc:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\requests\\sessions.py:746\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    743\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[1;32m--> 746\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32mc:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\requests\\models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\urllib3\\response.py:1060\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1060\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1062\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m   1063\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\urllib3\\response.py:949\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[0;32m    947\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[1;32m--> 949\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    951\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\urllib3\\response.py:873\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    870\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    875\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    876\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    883\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\urllib3\\response.py:856\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\http\\client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;66;03m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(amt)\n\u001b[1;32m--> 463\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b)[:n]\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;66;03m# and self.chunked\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\http\\client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    502\u001b[0m         b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(b)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength]\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m n \u001b[38;5;129;01mand\u001b[39;00m b:\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\ssl.py:1242\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1239\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1240\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1241\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1243\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\ssl.py:1100\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1099\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1100\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.initializers import Constant\n",
    "import numpy as np\n",
    "import requests, zipfile, os\n",
    "from io import BytesIO\n",
    "\n",
    "# Function to download and extract GloVe embeddings\n",
    "def download_glove_embeddings():\n",
    "    \"\"\"\n",
    "    Downloads and extracts GloVe embeddings from Stanford's website.\n",
    "    \"\"\"\n",
    "    url = \"https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with zipfile.ZipFile(BytesIO(response.content)) as zfile:\n",
    "            zfile.extractall(\"./glove\")\n",
    "            print(\"GloVe embeddings downloaded and extracted.\")\n",
    "    else:\n",
    "        print(\"Failed to download GloVe embeddings.\")\n",
    "\n",
    "# Example usage to download GloVe embeddings\n",
    "download_glove_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fa1b1817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "def load_glove_embeddings(filepath='glove/glove.6B.100d.txt'):\n",
    "    \"\"\"\n",
    "    Load the pre-trained GloVe embeddings from the specified file.\n",
    "    \n",
    "    Parameters:\n",
    "    - filepath: The path to the GloVe file.\n",
    "    \n",
    "    Returns:\n",
    "    - embeddings_index: A dictionary mapping words to their GloVe vector representations.\n",
    "    \"\"\"\n",
    "    embeddings_index = {}\n",
    "    try:\n",
    "        with open(filepath, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "        print(f'Found {len(embeddings_index)} word vectors.')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"'{filepath}' File not found. Please make sure you have run the previous cell to download GloVe embeddings.\")\n",
    "    \n",
    "    return embeddings_index\n",
    "\n",
    "# Example usage to load GloVe embeddings\n",
    "embeddings_index = load_glove_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b777220505635",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Load the pre-trained embeddings\n",
    "# embeddings_index = {}\n",
    "# try:\n",
    "#     with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             values = line.split()\n",
    "#             word = values[0]\n",
    "#             coefs = np.asarray(values[1:], dtype='float32')\n",
    "#             embeddings_index[word] = coefs\n",
    "#     print(f'Found {len(embeddings_index)} word vectors.')\n",
    "# except FileNotFoundError:\n",
    "#     print(\"'glove.6B.100d.txt' File not found. Please make sure you have ran the previous cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d3e48ff004757cf2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if VOCAB_SIZE is set\n",
    "if vocab_size is None:\n",
    "    print(\"You need to complete the previous parts of your assignment in order for this to work.\")\n",
    "else:\n",
    "    # Create an embedding matrix\n",
    "    embedding_matrix = np.zeros((vocab_size, 100))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if i < vocab_size:\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e3d21d5dbbbcf9f9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if VOCAB_SIZE is set\n",
    "if vocab_size is None:\n",
    "    print(\"You need to complete the previous parts of your assignment in order for this to work.\")\n",
    "else:\n",
    "    embedding_layer = Embedding(\n",
    "        vocab_size, 100, weights=[embedding_matrix], input_length=SEQ_LENGTH, trainable=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "236cb723e4e5b3fc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_10 (Embedding)    (None, 20, 100)           1401100   \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 20, 150)           150600    \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 20, 150)           0         \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 150)               180600    \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 150)               0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 150)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 14011)             2115661   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,847,961\n",
      "Trainable params: 3,847,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define a new model and train it\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Flatten\n",
    "\n",
    "def build_refined_model(vocab_size, max_sequence_len, embedding_dim=100, lstm_units=150, dropout_rate=0.2, embedding_matrix=None):\n",
    "    \"\"\"\n",
    "    Build a refined RNN model with options for pre-trained embeddings, additional LSTM layers, and dropout.\n",
    "    \n",
    "    Parameters:\n",
    "    - vocab_size: The size of the vocabulary.\n",
    "    - max_sequence_len: The maximum length of the input sequences.\n",
    "    - embedding_dim: The dimension of the embedding layer.\n",
    "    - lstm_units: The number of units in the LSTM layer.\n",
    "    - dropout_rate: The dropout rate to apply after LSTM layers.\n",
    "    - embedding_matrix: Pre-trained embedding weights (optional).\n",
    "    \n",
    "    Returns:\n",
    "    - The compiled Keras model.\n",
    "    \"\"\"\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Flatten\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Embedding layer (using pre-trained embeddings if provided)\n",
    "    if embedding_matrix is not None:\n",
    "        model.add(Embedding(input_dim=vocab_size, \n",
    "                            output_dim=embedding_dim, \n",
    "                            input_length=max_sequence_len, \n",
    "                            embeddings_initializer=Constant(embedding_matrix), \n",
    "                            trainable=False))\n",
    "    else:\n",
    "        model.add(Embedding(input_dim=vocab_size, \n",
    "                            output_dim=embedding_dim, \n",
    "                            input_length=max_sequence_len))\n",
    "    \n",
    "    # Add LSTM layer with dropout\n",
    "    model.add(LSTM(units=lstm_units, return_sequences=True))\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    \n",
    "    # Add another LSTM layer\n",
    "    model.add(LSTM(units=lstm_units))\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    \n",
    "    # Add Flatten layer\n",
    "    model.add(Flatten())\n",
    "\n",
    "\n",
    "    # Add Dense output layer with softmax activation\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage to build and compile a refined model\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "refined_model = build_refined_model(vocab_size, SEQ_LENGTH, embedding_dim=100, lstm_units=150, dropout_rate=0.2)\n",
    "refined_model.summary()\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "484d9165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19656\\2012194820.py\", line 1, in <module>\n      history = refined_model.fit(\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n      return self.compiled_loss(\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\keras\\losses.py\", line 272, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\keras\\losses.py\", line 2084, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\keras\\backend.py\", line 5630, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nlogits and labels must have the same first dimension, got logits shape [128,14011] and labels shape [640000]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_111265]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[108], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mrefined_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Print a message after training is complete\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel training complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19656\\2012194820.py\", line 1, in <module>\n      history = refined_model.fit(\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n      return self.compiled_loss(\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\keras\\losses.py\", line 272, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\keras\\losses.py\", line 2084, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"c:\\Users\\david\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\keras\\backend.py\", line 5630, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nlogits and labels must have the same first dimension, got logits shape [128,14011] and labels shape [640000]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_111265]"
     ]
    }
   ],
   "source": [
    "history = refined_model.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=5, \n",
    "    batch_size=128, \n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "\n",
    "# Print a message after training is complete\n",
    "print(\"Model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742095fb",
   "metadata": {},
   "source": [
    "## Criteria\n",
    "\n",
    "|Criteria|Complete|Incomplete|\n",
    "|----|----|----|\n",
    "|Task 1|The task has been completed successfully and there are no errors.|The task is still incomplete and there is at least one error.|\n",
    "|Task 2|The task has been completed successfully and there are no errors.|The task is still incomplete and there is at least one error.|\n",
    "|Task 3|The task has been completed successfully and there are no errors.|The task is still incomplete and there is at least one error.|\n",
    "|Task 4|The task has been completed successfully and there are no errors.|The task is still incomplete and there is at least one error.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1990e2d2",
   "metadata": {},
   "source": [
    "## Submission Information\n",
    "\n",
    "ðŸš¨**Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)**ðŸš¨ for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "### Submission Parameters:\n",
    "* Submission Due Date: `HH:MM AM/PM - DD/MM/YYYY`\n",
    "* The branch name for your repo should be: `assignment-2`\n",
    "* What to submit for this assignment:\n",
    "    * This Jupyter Notebook (assignment_2.ipynb) should be populated and should be the only change in your pull request.\n",
    "* What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/deep_learning/pull/<pr_id>`\n",
    "    * Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "Checklist:\n",
    "- [ ] Created a branch with the correct naming convention.\n",
    "- [ ] Ensured that the repository is public.\n",
    "- [ ] Reviewed the PR description guidelines and adhered to them.\n",
    "- [ ] Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack at `#cohort-3-help`. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
